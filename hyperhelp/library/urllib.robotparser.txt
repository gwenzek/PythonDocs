%hyperhelp title="urllib.robotparser" date="2021-07-11"
*|module-urllib.robotparser:⚓|*

*Source code:* |:github.com/python/cpython/tree/3.8/Lib/urllib/robotparser.py:Lib/urllib/robotparser.py|

*|index-0:⚓|*

======================================================================

This module provides a single class, |:urllib.robotparser.RobotFileParser:RobotFileParser|, which answers questions
about whether or not a particular user agent can fetch a URL on the Web site
that published the "robots.txt" file.  For more details on the structure of "
robots.txt" files, see |:www.robotstxt.org/orig.html:http://www.robotstxt.org/orig.html|.

*urllib.robotparser.RobotFileParser:class urllib.robotparser.RobotFileParser(url='')*

   This class provides methods to read, parse and answer questions about the "
   robots.txt" file at *url*.

   *urllib.robotparser.RobotFileParser.set_url:set_url(url)*

      Sets the URL referring to a "robots.txt" file.

   *urllib.robotparser.RobotFileParser.read:read()*

      Reads the "robots.txt" URL and feeds it to the parser.

   *urllib.robotparser.RobotFileParser.parse:parse(lines)*

      Parses the lines argument.

   *urllib.robotparser.RobotFileParser.can_fetch:can_fetch(useragent, url)*

      Returns "True" if the *useragent* is allowed to fetch the *url* according to the
      rules contained in the parsed "robots.txt" file.

   *urllib.robotparser.RobotFileParser.mtime:mtime()*

      Returns the time the "robots.txt" file was last fetched.  This is useful for
      long-running web spiders that need to check for new "robots.txt" files
      periodically.

   *urllib.robotparser.RobotFileParser.modified:modified()*

      Sets the time the "robots.txt" file was last fetched to the current time.

   *urllib.robotparser.RobotFileParser.crawl_delay:crawl_delay(useragent)*

      Returns the value of the "Crawl-delay" parameter from "robots.txt" for the *
      useragent* in question.  If there is no such parameter or it doesn’t apply to
      the *useragent* specified or the "robots.txt" entry for this parameter has
      invalid syntax, return "None".

      New in version 3.6.

   *urllib.robotparser.RobotFileParser.request_rate:request_rate(useragent)*

      Returns the contents of the "Request-rate" parameter from "robots.txt" as a
      |:glossary.txt/term-named-tuple:named tuple| "RequestRate(requests, seconds)". If there is no such parameter or
      it doesn’t apply to the *useragent* specified or the "robots.txt" entry for this
      parameter has invalid syntax, return "None".

      New in version 3.6.

   *urllib.robotparser.RobotFileParser.site_maps:site_maps()*

      Returns the contents of the "Sitemap" parameter from "robots.txt" in the form of
      a |:library/stdtypes.txt/list:list()|. If there is no such parameter or the "robots.txt" entry for this
      parameter has invalid syntax, return "None".

      New in version 3.8.

The following example demonstrates basic use of the |:urllib.robotparser.RobotFileParser:RobotFileParser| class:

```rst
>>> import urllib.robotparser
>>> rp = urllib.robotparser.RobotFileParser()
>>> rp.set_url("http://www.musi-cal.com/robots.txt")
>>> rp.read()
>>> rrate = rp.request_rate("*")
>>> rrate.requests
3
>>> rrate.seconds
20
>>> rp.crawl_delay("*")
6
>>> rp.can_fetch("*", "http://www.musi-cal.com/cgi-bin/search?city=San+Francisco")
False
>>> rp.can_fetch("*", "http://www.musi-cal.com/")
True
```



