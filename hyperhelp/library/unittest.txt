%hyperhelp title="unittest" date="2021-07-11"
*|module-unittest:⚓|*

*Source code:* |:github.com/python/cpython/tree/3.8/Lib/unittest/__init__.py:Lib/unittest/__init__.py|

======================================================================

(If you are already familiar with the basic concepts of testing, you might want
to skip to |:assert-methods:the list of assert methods|.)

The |:module-unittest:unittest| unit testing framework was originally inspired by JUnit and has a
similar flavor as major unit testing frameworks in other languages.  It supports
test automation, sharing of setup and shutdown code for tests, aggregation of
tests into collections, and independence of the tests from the reporting
framework.

To achieve this, |:module-unittest:unittest| supports some important concepts in an object-
oriented way:

test fixture
   A *test fixture* represents the preparation needed to perform one or more tests,
   and any associated cleanup actions.  This may involve, for example, creating
   temporary or proxy databases, directories, or starting a server process.

test case
   A *test case* is the individual unit of testing.  It checks for a specific
   response to a particular set of inputs.  |:module-unittest:unittest| provides a base class,
   |:unittest.TestCase:TestCase|, which may be used to create new test cases.

test suite
   A *test suite* is a collection of test cases, test suites, or both.  It is used
   to aggregate tests that should be executed together.

test runner
   A *test runner* is a component which orchestrates the execution of tests and
   provides the outcome to the user.  The runner may use a graphical interface, a
   textual interface, or return a special value to indicate the results of
   executing the tests.

See also:

  Module |:library/doctest.txt/module-doctest:doctest|
     Another test-support module with a very different flavor.

  |:www.xprogramming.com/testfram.htm:Simple Smalltalk Testing: With Patterns|*|simple-smalltalk-testing-with-patterns:⚓|*
     Kent Beck’s original paper on testing frameworks using the pattern shared by
     |:module-unittest:unittest|.

  |:docs.pytest.org:pytest|*|pytest:⚓|*
     Third-party unittest framework with a lighter-weight syntax for writing tests.
     For example, "assert func(10) == 42".

  |:wiki.python.org/moin/PythonTestingToolsTaxonomy:The Python Testing Tools Taxonomy|*|the-python-testing-tools-taxonomy:⚓|*
     An extensive list of Python testing tools including functional testing
     frameworks and mock object libraries.

  |:lists.idyll.org/listinfo/testing-in-python:Testing in Python Mailing List|*|testing-in-python-mailing-list:⚓|*
     A special-interest-group for discussion of testing, and testing tools, in
     Python.

  The script "Tools/unittestgui/unittestgui.py" in the Python source distribution
  is a GUI tool for test discovery and execution.  This is intended largely for
  ease of use for those new to unit testing.  For production environments it is
  recommended that tests be driven by a continuous integration system such as
  |:buildbot.net:Buildbot|*|buildbot:⚓|* , |:jenkins.io:Jenkins|*|jenkins:⚓|*  or |:travis-ci.com:Travis-CI|*|travis-ci:⚓|*
  , or |:www.appveyor.com:AppVeyor|*|appveyor:⚓|* .

# basic-example:Basic example

The |:module-unittest:unittest| module provides a rich set of tools for constructing and running
tests.  This section demonstrates that a small subset of the tools suffice to
meet the needs of most users.

Here is a short script to test three string methods:

```rst
import unittest

class TestStringMethods(unittest.TestCase):

    def test_upper(self):
        self.assertEqual('foo'.upper(), 'FOO')

    def test_isupper(self):
        self.assertTrue('FOO'.isupper())
        self.assertFalse('Foo'.isupper())

    def test_split(self):
        s = 'hello world'
        self.assertEqual(s.split(), ['hello', 'world'])
        # check that s.split fails when the separator is not a string
        with self.assertRaises(TypeError):
            s.split(2)

if __name__ == '__main__':
    unittest.main()
```

A testcase is created by subclassing |:unittest.TestCase:unittest.TestCase|.  The three individual
tests are defined with methods whose names start with the letters "test".  This
naming convention informs the test runner about which methods represent tests.

The crux of each test is a call to |:unittest.TestCase.assertEqual:assertEqual()| to check for an expected
result; |:unittest.TestCase.assertTrue:assertTrue()| or |:unittest.TestCase.assertFalse:assertFalse()| to verify a condition; or
|:unittest.TestCase.assertRaises:assertRaises()| to verify that a specific exception gets raised.  These methods
are used instead of the |:reference/simple_stmts.txt/assert:assert| statement so the test runner can accumulate all
test results and produce a report.

The |:unittest.TestCase.setUp:setUp()| and |:unittest.TestCase.tearDown:tearDown()| methods allow you to define instructions that
will be executed before and after each test method. They are covered in more
detail in the section |:organizing-tests:Organizing test code|.

The final block shows a simple way to run the tests. |:unittest.main:unittest.main()| provides
a command-line interface to the test script.  When run from the command line,
the above script produces an output that looks like this:

```rst
...
----------------------------------------------------------------------
Ran 3 tests in 0.000s

OK
```

Passing the "-v" option to your test script will instruct |:unittest.main:unittest.main()| to
enable a higher level of verbosity, and produce the following output:

```rst
test_isupper (__main__.TestStringMethods) ... ok
test_split (__main__.TestStringMethods) ... ok
test_upper (__main__.TestStringMethods) ... ok

----------------------------------------------------------------------
Ran 3 tests in 0.001s

OK
```

The above examples show the most commonly used |:module-unittest:unittest| features which are
sufficient to meet many everyday testing needs.  The remainder of the
documentation explores the full feature set from first principles.

*|unittest-command-line-interface:⚓|* # command-line-interface:Command-Line
Interface

The unittest module can be used from the command line to run tests from modules,
classes or even individual test methods:

```rst
python -m unittest test_module1 test_module2
python -m unittest test_module.TestClass
python -m unittest test_module.TestClass.test_method
```

You can pass in a list with any combination of module names, and fully qualified
class or method names.

Test modules can be specified by file path as well:

```rst
python -m unittest tests/test_something.py
```

This allows you to use the shell filename completion to specify the test module.
The file specified must still be importable as a module. The path is converted
to a module name by removing the ‘.py’ and converting path separators into ‘.’.
If you want to execute a test file that isn’t importable as a module you should
execute the file directly instead.

You can run tests with more detail (higher verbosity) by passing in the -v flag:

```rst
python -m unittest -v test_module
```

When executed without arguments |:unittest-test-discovery:Test Discovery| is started:

```rst
python -m unittest
```

For a list of all the command-line options:

```rst
python -m unittest -h
```

Changed in version 3.2: In earlier versions it was only possible to run
individual test methods and not modules or classes.

## command-line-options:Command-line options

*unittest* supports these command-line options:

*cmdoption-unittest-b:-b, --buffer*

   The standard output and standard error streams are buffered during the test run.
   Output during a passing test is discarded. Output is echoed normally on test
   fail or error and is added to the failure messages.

*cmdoption-unittest-c:-c, --catch*

   "Control-C" during the test run waits for the current test to end and then
   reports all the results so far. A second "Control-C" raises the normal
   |:library/exceptions.txt/KeyboardInterrupt:KeyboardInterrupt| exception.

   See |:signal-handling:Signal Handling| for the functions that provide this functionality.

*cmdoption-unittest-f:-f, --failfast*

   Stop the test run on the first error or failure.

*cmdoption-unittest-k:-k*

   Only run test methods and classes that match the pattern or substring. This
   option may be used multiple times, in which case all test cases that match of
   the given patterns are included.

   Patterns that contain a wildcard character ("*") are matched against the test
   name using |:library/fnmatch.txt/fnmatch.fnmatchcase:fnmatch.fnmatchcase()|; otherwise simple case-sensitive substring
   matching is used.

   Patterns are matched against the fully qualified test method name as imported by
   the test loader.

   For example, "-k foo" matches "foo_tests.SomeTest.test_something", "
   bar_tests.SomeTest.test_foo", but not "bar_tests.FooTest.test_something".

*cmdoption-unittest-locals:--locals*

   Show local variables in tracebacks.

New in version 3.2: The command-line options "-b", "-c" and "-f" were added.

New in version 3.5: The command-line option "--locals".

New in version 3.7: The command-line option "-k".

The command line can also be used for test discovery, for running all of the
tests in a project or just a subset.

*|unittest-test-discovery:⚓|* # test-discovery:Test Discovery

New in version 3.2.

Unittest supports simple test discovery. In order to be compatible with test
discovery, all of the test files must be |:tutorial/modules.txt/tut-modules:modules| or |:tutorial/modules.txt/tut-packages:packages| (including
|:glossary.txt/term-namespace-package:namespace packages|) importable from the top-level directory of the project
(this means that their filenames must be valid |:reference/lexical_analysis.txt/identifiers:identifiers|).

Test discovery is implemented in |:unittest.TestLoader.discover:TestLoader.discover()|, but can also be used
from the command line. The basic command-line usage is:

```rst
cd project_directory
python -m unittest discover
```

Note:

  As a shortcut, "python -m unittest" is the equivalent of "python -m unittest
  discover". If you want to pass arguments to test discovery the "discover" sub-
  command must be used explicitly.

The "discover" sub-command has the following options:

*cmdoption-unittest-discover-v:-v, --verbose*

   Verbose output

*cmdoption-unittest-discover-s:-s, --start-directory directory*

   Directory to start discovery ("." default)

*cmdoption-unittest-discover-p:-p, --pattern pattern*

   Pattern to match test files ("test*.py" default)

*cmdoption-unittest-discover-t:-t, --top-level-directory directory*

   Top level directory of project (defaults to start directory)

The |:cmdoption-unittest-discover-s:-s|, |:cmdoption-unittest-discover-p:-p|, and |:cmdoption-unittest-discover-t:-t| options can be passed in as positional arguments in
that order. The following two command lines are equivalent:

```rst
python -m unittest discover -s project_directory -p "*_test.py"
python -m unittest discover project_directory "*_test.py"
```

As well as being a path it is possible to pass a package name, for example "
myproject.subpackage.test", as the start directory. The package name you supply
will then be imported and its location on the filesystem will be used as the
start directory.

Caution:

  Test discovery loads tests by importing them. Once test discovery has found all
  the test files from the start directory you specify it turns the paths into
  package names to import. For example "foo/bar/baz.py" will be imported as "
  foo.bar.baz".If you have a package installed globally and attempt test discovery
  on a different copy of the package then the import *could* happen from the wrong
  place. If this happens test discovery will warn you and exit.If you supply the
  start directory as a package name rather than a path to a directory then
  discover assumes that whichever location it imports from is the location you
  intended, so you will not get the warning.

Test modules and packages can customize test loading and discovery by through
the |:load-tests-protocol:load_tests protocol|.

Changed in version 3.4: Test discovery supports |:glossary.txt/term-namespace-package:namespace packages|.

*|organizing-tests:⚓|* # organizing-test-code:Organizing test code

The basic building blocks of unit testing are *test cases* — single scenarios
that must be set up and checked for correctness.  In |:module-unittest:unittest|, test cases are
represented by |:unittest.TestCase:unittest.TestCase| instances. To make your own test cases you
must write subclasses of |:unittest.TestCase:TestCase| or use |:unittest.FunctionTestCase:FunctionTestCase|.

The testing code of a |:unittest.TestCase:TestCase| instance should be entirely self contained,
such that it can be run either in isolation or in arbitrary combination with any
number of other test cases.

The simplest |:unittest.TestCase:TestCase| subclass will simply implement a test method (i.e. a
method whose name starts with "test") in order to perform specific testing code:

```rst
import unittest

class DefaultWidgetSizeTestCase(unittest.TestCase):
    def test_default_widget_size(self):
        widget = Widget('The widget')
        self.assertEqual(widget.size(), (50, 50))
```

Note that in order to test something, we use one of the "assert*()" methods
provided by the |:unittest.TestCase:TestCase| base class.  If the test fails, an exception will be
raised with an explanatory message, and |:module-unittest:unittest| will identify the test case
as a *failure*.  Any other exceptions will be treated as *errors*.

Tests can be numerous, and their set-up can be repetitive.  Luckily, we can
factor out set-up code by implementing a method called |:unittest.TestCase.setUp:setUp()|, which the
testing framework will automatically call for every single test we run:

```rst
import unittest

class WidgetTestCase(unittest.TestCase):
    def setUp(self):
        self.widget = Widget('The widget')

    def test_default_widget_size(self):
        self.assertEqual(self.widget.size(), (50,50),
                         'incorrect default size')

    def test_widget_resize(self):
        self.widget.resize(100,150)
        self.assertEqual(self.widget.size(), (100,150),
                         'wrong size after resize')
```

Note:

  The order in which the various tests will be run is determined by sorting the
  test method names with respect to the built-in ordering for strings.

If the |:unittest.TestCase.setUp:setUp()| method raises an exception while the test is running, the
framework will consider the test to have suffered an error, and the test method
will not be executed.

Similarly, we can provide a |:unittest.TestCase.tearDown:tearDown()| method that tidies up after the test
method has been run:

```rst
import unittest

class WidgetTestCase(unittest.TestCase):
    def setUp(self):
        self.widget = Widget('The widget')

    def tearDown(self):
        self.widget.dispose()
```

If |:unittest.TestCase.setUp:setUp()| succeeded, |:unittest.TestCase.tearDown:tearDown()| will be run whether the test method
succeeded or not.

Such a working environment for the testing code is called a *test fixture*.  A
new TestCase instance is created as a unique test fixture used to execute each
individual test method.  Thus |:unittest.TestCase.setUp:setUp()|, |:unittest.TestCase.tearDown:tearDown()|, and "__init__()" will be
called once per test.

It is recommended that you use TestCase implementations to group tests together
according to the features they test.  |:module-unittest:unittest| provides a mechanism for this:
the *test suite*, represented by |:module-unittest:unittest|’s |:unittest.TestSuite:TestSuite| class.  In most cases,
calling |:unittest.main:unittest.main()| will do the right thing and collect all the module’s
test cases for you and execute them.

However, should you want to customize the building of your test suite, you can
do it yourself:

```rst
def suite():
    suite = unittest.TestSuite()
    suite.addTest(WidgetTestCase('test_default_widget_size'))
    suite.addTest(WidgetTestCase('test_widget_resize'))
    return suite

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite())
```

You can place the definitions of test cases and test suites in the same modules
as the code they are to test (such as "widget.py"), but there are several
advantages to placing the test code in a separate module, such as "
test_widget.py":

* The test module can be run standalone from the command line.

* The test code can more easily be separated from shipped code.

* There is less temptation to change test code to fit the code it tests without a
  good reason.

* Test code should be modified much less frequently than the code it tests.

* Tested code can be refactored more easily.

* Tests for modules written in C must be in separate modules anyway, so why not be
  consistent?

* If the testing strategy changes, there is no need to change the source code.

*|legacy-unit-tests:⚓|* # re-using-old-test-code:Re-using old test code

Some users will find that they have existing test code that they would like to
run from |:module-unittest:unittest|, without converting every old test function to a |:unittest.TestCase:TestCase|
subclass.

For this reason, |:module-unittest:unittest| provides a |:unittest.FunctionTestCase:FunctionTestCase| class. This subclass
of |:unittest.TestCase:TestCase| can be used to wrap an existing test function.  Set-up and tear-
down functions can also be provided.

Given the following test function:

```rst
def testSomething():
    something = makeSomething()
    assert something.name is not None
    # ...
```

one can create an equivalent test case instance as follows, with optional set-up
and tear-down methods:

```rst
testcase = unittest.FunctionTestCase(testSomething,
                                     setUp=makeSomethingDB,
                                     tearDown=deleteSomethingDB)
```

Note:

  Even though |:unittest.FunctionTestCase:FunctionTestCase| can be used to quickly convert an existing test
  base over to a |:module-unittest:unittest|-based system, this approach is not recommended.
  Taking the time to set up proper |:unittest.TestCase:TestCase| subclasses will make future test
  refactorings infinitely easier.

In some cases, the existing tests may have been written using the |:library/doctest.txt/module-doctest:doctest|
module.  If so, |:library/doctest.txt/module-doctest:doctest| provides a "DocTestSuite" class that can automatically
build |:unittest.TestSuite:unittest.TestSuite| instances from the existing |:library/doctest.txt/module-doctest:doctest|-based tests.

*|unittest-skipping:⚓|* # skipping-tests-and-expected-failures:Skipping tests
and expected failures

New in version 3.1.

Unittest supports skipping individual test methods and even whole classes of
tests.  In addition, it supports marking a test as an “expected failure,” a test
that is broken and will fail, but shouldn’t be counted as a failure on a
|:unittest.TestResult:TestResult|.

Skipping a test is simply a matter of using the |:unittest.skip:skip()| |:glossary.txt/term-decorator:decorator| or one of
its conditional variants, calling |:unittest.TestCase.skipTest:TestCase.skipTest()| within a |:unittest.TestCase.setUp:setUp()| or
test method, or raising |:unittest.SkipTest:SkipTest| directly.

Basic skipping looks like this:

```rst
class MyTestCase(unittest.TestCase):

    @unittest.skip("demonstrating skipping")
    def test_nothing(self):
        self.fail("shouldn't happen")

    @unittest.skipIf(mylib.__version__ < (1, 3),
                     "not supported in this library version")
    def test_format(self):
        # Tests that work for only a certain version of the library.
        pass

    @unittest.skipUnless(sys.platform.startswith("win"), "requires Windows")
    def test_windows_support(self):
        # windows specific testing code
        pass

    def test_maybe_skipped(self):
        if not external_resource_available():
            self.skipTest("external resource not available")
        # test code that depends on the external resource
        pass
```

This is the output of running the example above in verbose mode:

```rst
test_format (__main__.MyTestCase) ... skipped 'not supported in this library version'
test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'
test_maybe_skipped (__main__.MyTestCase) ... skipped 'external resource not available'
test_windows_support (__main__.MyTestCase) ... skipped 'requires Windows'

----------------------------------------------------------------------
Ran 4 tests in 0.005s

OK (skipped=4)
```

Classes can be skipped just like methods:

```rst
@unittest.skip("showing class skipping")
class MySkippedTestCase(unittest.TestCase):
    def test_not_run(self):
        pass
```

|:unittest.TestCase.setUp:TestCase.setUp()| can also skip the test.  This is useful when a resource that
needs to be set up is not available.

Expected failures use the |:unittest.expectedFailure:expectedFailure()| decorator.

```rst
class ExpectedFailureTestCase(unittest.TestCase):
    @unittest.expectedFailure
    def test_fail(self):
        self.assertEqual(1, 0, "broken")
```

It’s easy to roll your own skipping decorators by making a decorator that calls
|:unittest.skip:skip()| on the test when it wants it to be skipped.  This decorator skips the
test unless the passed object has a certain attribute:

```rst
def skipUnlessHasattr(obj, attr):
    if hasattr(obj, attr):
        return lambda func: func
    return unittest.skip("{!r} doesn't have {!r}".format(obj, attr))
```

The following decorators and exception implement test skipping and expected
failures:

*unittest.skip:@unittest.skip(reason)*

   Unconditionally skip the decorated test.  *reason* should describe why the test
   is being skipped.

*unittest.skipIf:@unittest.skipIf(condition, reason)*

   Skip the decorated test if *condition* is true.

*unittest.skipUnless:@unittest.skipUnless(condition, reason)*

   Skip the decorated test unless *condition* is true.

*unittest.expectedFailure:@unittest.expectedFailure*

   Mark the test as an expected failure or error.  If the test fails or errors it
   will be considered a success.  If the test passes, it will be considered a
   failure.

*unittest.SkipTest:exception unittest.SkipTest(reason)*

   This exception is raised to skip a test.

   Usually you can use |:unittest.TestCase.skipTest:TestCase.skipTest()| or one of the skipping decorators
   instead of raising this directly.

Skipped tests will not have |:unittest.TestCase.setUp:setUp()| or |:unittest.TestCase.tearDown:tearDown()| run around them. Skipped
classes will not have |:unittest.TestCase.setUpClass:setUpClass()| or |:unittest.TestCase.tearDownClass:tearDownClass()| run. Skipped modules
will not have "setUpModule()" or "tearDownModule()" run.

*|subtests:⚓|* # distinguishing-test-iterations-using-subtests:Distinguishing
test iterations using subtests

New in version 3.4.

When there are very small differences among your tests, for instance some
parameters, unittest allows you to distinguish them inside the body of a test
method using the |:unittest.TestCase.subTest:subTest()| context manager.

For example, the following test:

```rst
class NumbersTest(unittest.TestCase):

    def test_even(self):
        """
        Test that numbers between 0 and 5 are all even.
        """
        for i in range(0, 6):
            with self.subTest(i=i):
                self.assertEqual(i % 2, 0)
```

will produce the following output:

```rst
======================================================================
FAIL: test_even (__main__.NumbersTest) (i=1)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "subtests.py", line 32, in test_even
    self.assertEqual(i % 2, 0)
AssertionError: 1 != 0

======================================================================
FAIL: test_even (__main__.NumbersTest) (i=3)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "subtests.py", line 32, in test_even
    self.assertEqual(i % 2, 0)
AssertionError: 1 != 0

======================================================================
FAIL: test_even (__main__.NumbersTest) (i=5)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "subtests.py", line 32, in test_even
    self.assertEqual(i % 2, 0)
AssertionError: 1 != 0
```

Without using a subtest, execution would stop after the first failure, and the
error would be less easy to diagnose because the value of "i" wouldn’t be
displayed:

```rst
======================================================================
FAIL: test_even (__main__.NumbersTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "subtests.py", line 32, in test_even
    self.assertEqual(i % 2, 0)
AssertionError: 1 != 0
```

*|unittest-contents:⚓|* # classes-and-functions:Classes and functions

This section describes in depth the API of |:module-unittest:unittest|.

## test-cases:Test cases

*unittest.TestCase:class unittest.TestCase(methodName='runTest')*

   Instances of the |:unittest.TestCase:TestCase| class represent the logical test units in the
   |:module-unittest:unittest| universe.  This class is intended to be used as a base class, with
   specific tests being implemented by concrete subclasses.  This class implements
   the interface needed by the test runner to allow it to drive the tests, and
   methods that the test code can use to check for and report various kinds of
   failure.

   Each instance of |:unittest.TestCase:TestCase| will run a single base method: the method named *
   methodName*. In most uses of |:unittest.TestCase:TestCase|, you will neither change the *methodName
   * nor reimplement the default "runTest()" method.

   Changed in version 3.2: |:unittest.TestCase:TestCase| can be instantiated successfully without
   providing a *methodName*. This makes it easier to experiment with |:unittest.TestCase:TestCase|
   from the interactive interpreter.

   |:unittest.TestCase:TestCase| instances provide three groups of methods: one group used to run the
   test, another used by the test implementation to check conditions and report
   failures, and some inquiry methods allowing information about the test itself to
   be gathered.

   Methods in the first group (running the test) are:

   *unittest.TestCase.setUp:setUp()*

      Method called to prepare the test fixture.  This is called immediately before
      calling the test method; other than |:library/exceptions.txt/AssertionError:AssertionError| or |:unittest.SkipTest:SkipTest|, any
      exception raised by this method will be considered an error rather than a test
      failure. The default implementation does nothing.

   *unittest.TestCase.tearDown:tearDown()*

      Method called immediately after the test method has been called and the result
      recorded.  This is called even if the test method raised an exception, so the
      implementation in subclasses may need to be particularly careful about checking
      internal state.  Any exception, other than |:library/exceptions.txt/AssertionError:AssertionError| or |:unittest.SkipTest:SkipTest|,
      raised by this method will be considered an additional error rather than a test
      failure (thus increasing the total number of reported errors). This method will
      only be called if the |:unittest.TestCase.setUp:setUp()| succeeds, regardless of the outcome of the test
      method. The default implementation does nothing.

   *unittest.TestCase.setUpClass:setUpClass()*

      A class method called before tests in an individual class are run. "setUpClass"
      is called with the class as the only argument and must be decorated as a
      |:library/functions.txt/classmethod:classmethod()|:

```rst
@classmethod
def setUpClass(cls):
    ...
```

      See |:class-and-module-fixtures:Class and Module Fixtures| for more details.

      New in version 3.2.

   *unittest.TestCase.tearDownClass:tearDownClass()*

      A class method called after tests in an individual class have run. "
      tearDownClass" is called with the class as the only argument and must be
      decorated as a |:library/functions.txt/classmethod:classmethod()|:

```rst
@classmethod
def tearDownClass(cls):
    ...
```

      See |:class-and-module-fixtures:Class and Module Fixtures| for more details.

      New in version 3.2.

   *unittest.TestCase.run:run(result=None)*

      Run the test, collecting the result into the |:unittest.TestResult:TestResult| object passed as *
      result*.  If *result* is omitted or "None", a temporary result object is created
      (by calling the |:unittest.TestCase.defaultTestResult:defaultTestResult()| method) and used. The result object is
      returned to |:unittest.TestCase.run:run()|’s caller.

      The same effect may be had by simply calling the |:unittest.TestCase:TestCase| instance.

      Changed in version 3.3: Previous versions of "run" did not return the result.
      Neither did calling an instance.

   *unittest.TestCase.skipTest:skipTest(reason)*

      Calling this during a test method or |:unittest.TestCase.setUp:setUp()| skips the current test.  See
      |:unittest-skipping:Skipping tests and expected failures| for more information.

      New in version 3.1.

   *unittest.TestCase.subTest:subTest(msg=None, **params)*

      Return a context manager which executes the enclosed code block as a subtest.  *
      msg* and *params* are optional, arbitrary values which are displayed whenever a
      subtest fails, allowing you to identify them clearly.

      A test case can contain any number of subtest declarations, and they can be
      arbitrarily nested.

      See |:subtests:Distinguishing test iterations using subtests| for more information.

      New in version 3.4.

   *unittest.TestCase.debug:debug()*

      Run the test without collecting the result.  This allows exceptions raised by
      the test to be propagated to the caller, and can be used to support running
      tests under a debugger.

   *|assert-methods:⚓|*

   The |:unittest.TestCase:TestCase| class provides several assert methods to check for and report
   failures.  The following table lists the most commonly used methods (see the
   tables below for more assert methods):

   +-------------------------------------------+-------------------------------+-----------------+
   | Method                                    | Checks that                   | New in          |
   |===========================================|===============================|=================|
   | |:unittest.TestCase.assertEqual:assertEq  | "a == b"                      |                 |
   | ual(a, b)|                                |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertNotEqual:asser  | "a != b"                      |                 |
   | tNotEqual(a, b)|                          |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertTrue:assertTru  | "bool(x) is True"             |                 |
   | e(x)|                                     |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertFalse:assertFa  | "bool(x) is False"            |                 |
   | lse(x)|                                   |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertIs:assertIs(a,  | "a is b"                      | 3.1             |
   | b)|                                       |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertIsNot:assertIs  | "a is not b"                  | 3.1             |
   | Not(a, b)|                                |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertIsNone:assertI  | "x is None"                   | 3.1             |
   | sNone(x)|                                 |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertIsNotNone:asse  | "x is not None"               | 3.1             |
   | rtIsNotNone(x)|                           |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertIn:assertIn(a,  | "a in b"                      | 3.1             |
   | b)|                                       |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertNotIn:assertNo  | "a not in b"                  | 3.1             |
   | tIn(a, b)|                                |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertIsInstance:ass  | "isinstance(a, b)"            | 3.2             |
   | ertIsInstance(a, b)|                      |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+
   | |:unittest.TestCase.assertNotIsInstance:  | "not isinstance(a, b)"        | 3.2             |
   | assertNotIsInstance(a, b)|                |                               |                 |
   +-------------------------------------------+-------------------------------+-----------------+

   All the assert methods accept a *msg* argument that, if specified, is used as
   the error message on failure (see also |:unittest.TestCase.longMessage:longMessage|). Note that the *msg*
   keyword argument can be passed to |:unittest.TestCase.assertRaises:assertRaises()|, |:unittest.TestCase.assertRaisesRegex:assertRaisesRegex()|,
   |:unittest.TestCase.assertWarns:assertWarns()|, |:unittest.TestCase.assertWarnsRegex:assertWarnsRegex()| only when they are used as a context
   manager.

   *unittest.TestCase.assertEqual:assertEqual(first, second, msg=None)*

      Test that *first* and *second* are equal.  If the values do not compare equal,
      the test will fail.

      In addition, if *first* and *second* are the exact same type and one of list,
      tuple, dict, set, frozenset or str or any type that a subclass registers with
      |:unittest.TestCase.addTypeEqualityFunc:addTypeEqualityFunc()| the type-specific equality function will be called in
      order to generate a more useful default error message (see also the
      |:type-specific-methods:list of type-specific methods|).

      Changed in version 3.1: Added the automatic calling of type-specific equality
      function.

      Changed in version 3.2: |:unittest.TestCase.assertMultiLineEqual:assertMultiLineEqual()| added as the default type
      equality function for comparing strings.

   *unittest.TestCase.assertNotEqual:assertNotEqual(first, second, msg=None)*

      Test that *first* and *second* are not equal.  If the values do compare equal,
      the test will fail.

   *unittest.TestCase.assertTrue:assertTrue(expr, msg=None)*
   *unittest.TestCase.assertFalse:assertFalse(expr, msg=None)*

      Test that *expr* is true (or false).

      Note that this is equivalent to "bool(expr) is True" and not to "expr is True"
      (use "assertIs(expr, True)" for the latter).  This method should also be avoided
      when more specific methods are available (e.g. "assertEqual(a, b)" instead of "
      assertTrue(a == b)"), because they provide a better error message in case of
      failure.

   *unittest.TestCase.assertIs:assertIs(first, second, msg=None)*
   *unittest.TestCase.assertIsNot:assertIsNot(first, second, msg=None)*

      Test that *first* and *second* are (or are not) the same object.

      New in version 3.1.

   *unittest.TestCase.assertIsNone:assertIsNone(expr, msg=None)*
   *unittest.TestCase.assertIsNotNone:assertIsNotNone(expr, msg=None)*

      Test that *expr* is (or is not) "None".

      New in version 3.1.

   *unittest.TestCase.assertIn:assertIn(member, container, msg=None)*
   *unittest.TestCase.assertNotIn:assertNotIn(member, container, msg=None)*

      Test that *member* is (or is not) in *container*.

      New in version 3.1.

   *unittest.TestCase.assertIsInstance:assertIsInstance(obj, cls, msg=None)*
   *unittest.TestCase.assertNotIsInstance:assertNotIsInstance(obj, cls, msg=None)*

      Test that *obj* is (or is not) an instance of *cls* (which can be a class or a
      tuple of classes, as supported by |:library/functions.txt/isinstance:isinstance()|). To check for the exact type,
      use |:unittest.TestCase.assertIs:assertIs(type(obj), cls)|.

      New in version 3.2.

   It is also possible to check the production of exceptions, warnings, and log
   messages using the following methods:

   +-----------------------------------------------------------+----------------------------------------+--------------+
   | Method                                                    | Checks that                            | New in       |
   |===========================================================|========================================|==============|
   | |:unittest.TestCase.assertRaises:assertRaises(exc, fun,   | "fun(*args, **kwds)" raises *exc*      |              |
   | *args, **kwds)|                                           |                                        |              |
   +-----------------------------------------------------------+----------------------------------------+--------------+
   | |:unittest.TestCase.assertRaisesRegex:assertRaisesRegex(  | "fun(*args, **kwds)" raises *exc* and  | 3.1          |
   | exc, r, fun, *args, **kwds)|                              | the message matches regex *r*          |              |
   +-----------------------------------------------------------+----------------------------------------+--------------+
   | |:unittest.TestCase.assertWarns:assertWarns(warn, fun,    | "fun(*args, **kwds)" raises *warn*     | 3.2          |
   | *args, **kwds)|                                           |                                        |              |
   +-----------------------------------------------------------+----------------------------------------+--------------+
   | |:unittest.TestCase.assertWarnsRegex:assertWarnsRegex(wa  | "fun(*args, **kwds)" raises *warn* and | 3.2          |
   | rn, r, fun, *args, **kwds)|                               | the message matches regex *r*          |              |
   +-----------------------------------------------------------+----------------------------------------+--------------+
   | |:unittest.TestCase.assertLogs:assertLogs(logger, level)| | The "with" block logs on *logger* with | 3.4          |
   |                                                           | minimum *level*                        |              |
   +-----------------------------------------------------------+----------------------------------------+--------------+

   *unittest.TestCase.assertRaises:assertRaises(exception, callable, *args, **kwds)*
   assertRaises(exception, *, msg=None)

      Test that an exception is raised when *callable* is called with any positional
      or keyword arguments that are also passed to |:unittest.TestCase.assertRaises:assertRaises()|.  The test passes
      if *exception* is raised, is an error if another exception is raised, or fails
      if no exception is raised. To catch any of a group of exceptions, a tuple
      containing the exception classes may be passed as *exception*.

      If only the *exception* and possibly the *msg* arguments are given, return a
      context manager so that the code under test can be written inline rather than as
      a function:

```rst
with self.assertRaises(SomeException):
    do_something()
```

      When used as a context manager, |:unittest.TestCase.assertRaises:assertRaises()| accepts the additional keyword
      argument *msg*.

      The context manager will store the caught exception object in its "exception"
      attribute.  This can be useful if the intention is to perform additional checks
      on the exception raised:

```rst
with self.assertRaises(SomeException) as cm:
    do_something()

the_exception = cm.exception
self.assertEqual(the_exception.error_code, 3)
```

      Changed in version 3.1: Added the ability to use |:unittest.TestCase.assertRaises:assertRaises()| as a context
      manager.

      Changed in version 3.2: Added the "exception" attribute.

      Changed in version 3.3: Added the *msg* keyword argument when used as a context
      manager.

   *unittest.TestCase.assertRaisesRegex:assertRaisesRegex(exception, regex, callable, *args, **kwds)*
   assertRaisesRegex(exception, regex, *, msg=None)

      Like |:unittest.TestCase.assertRaises:assertRaises()| but also tests that *regex* matches on the string
      representation of the raised exception.  *regex* may be a regular expression
      object or a string containing a regular expression suitable for use by
      |:library/re.txt/re.search:re.search()|.  Examples:

```rst
self.assertRaisesRegex(ValueError, "invalid literal for.*XYZ'$",
                       int, 'XYZ')
```

      or:

```rst
with self.assertRaisesRegex(ValueError, 'literal'):
   int('XYZ')
```

      New in version 3.1: Added under the name "assertRaisesRegexp".

      Changed in version 3.2: Renamed to |:unittest.TestCase.assertRaisesRegex:assertRaisesRegex()|.

      Changed in version 3.3: Added the *msg* keyword argument when used as a context
      manager.

   *unittest.TestCase.assertWarns:assertWarns(warning, callable, *args, **kwds)*
   assertWarns(warning, *, msg=None)

      Test that a warning is triggered when *callable* is called with any positional
      or keyword arguments that are also passed to |:unittest.TestCase.assertWarns:assertWarns()|.  The test passes
      if *warning* is triggered and fails if it isn’t.  Any exception is an error. To
      catch any of a group of warnings, a tuple containing the warning classes may be
      passed as *warnings*.

      If only the *warning* and possibly the *msg* arguments are given, return a
      context manager so that the code under test can be written inline rather than as
      a function:

```rst
with self.assertWarns(SomeWarning):
    do_something()
```

      When used as a context manager, |:unittest.TestCase.assertWarns:assertWarns()| accepts the additional keyword
      argument *msg*.

      The context manager will store the caught warning object in its "warning"
      attribute, and the source line which triggered the warnings in the "filename"
      and "lineno" attributes. This can be useful if the intention is to perform
      additional checks on the warning caught:

```rst
with self.assertWarns(SomeWarning) as cm:
    do_something()

self.assertIn('myfile.py', cm.filename)
self.assertEqual(320, cm.lineno)
```

      This method works regardless of the warning filters in place when it is called.

      New in version 3.2.

      Changed in version 3.3: Added the *msg* keyword argument when used as a context
      manager.

   *unittest.TestCase.assertWarnsRegex:assertWarnsRegex(warning, regex, callable, *args, **kwds)*
   assertWarnsRegex(warning, regex, *, msg=None)

      Like |:unittest.TestCase.assertWarns:assertWarns()| but also tests that *regex* matches on the message of the
      triggered warning.  *regex* may be a regular expression object or a string
      containing a regular expression suitable for use by |:library/re.txt/re.search:re.search()|.  Example:

```rst
self.assertWarnsRegex(DeprecationWarning,
                      r'legacy_function\(\) is deprecated',
                      legacy_function, 'XYZ')
```

      or:

```rst
with self.assertWarnsRegex(RuntimeWarning, 'unsafe frobnicating'):
    frobnicate('/etc/passwd')
```

      New in version 3.2.

      Changed in version 3.3: Added the *msg* keyword argument when used as a context
      manager.

   *unittest.TestCase.assertLogs:assertLogs(logger=None, level=None)*

      A context manager to test that at least one message is logged on the *logger* or
      one of its children, with at least the given *level*.

      If given, *logger* should be a |:library/logging.txt/logging.Logger:logging.Logger| object or a |:library/stdtypes.txt/str:str| giving the
      name of a logger.  The default is the root logger, which will catch all messages
      that were not blocked by a non-propagating descendent logger.

      If given, *level* should be either a numeric logging level or its string
      equivalent (for example either ""ERROR"" or "logging.ERROR").  The default is "
      logging.INFO".

      The test passes if at least one message emitted inside the "with" block matches
      the *logger* and *level* conditions, otherwise it fails.

      The object returned by the context manager is a recording helper which keeps
      tracks of the matching log messages.  It has two attributes:

      *unittest.TestCase.records:records*

         A list of |:library/logging.txt/logging.LogRecord:logging.LogRecord| objects of the matching log messages.

      *unittest.TestCase.output:output*

         A list of |:library/stdtypes.txt/str:str| objects with the formatted output of matching messages.

      Example:

```rst
with self.assertLogs('foo', level='INFO') as cm:
   logging.getLogger('foo').info('first message')
   logging.getLogger('foo.bar').error('second message')
self.assertEqual(cm.output, ['INFO:foo:first message',
                             'ERROR:foo.bar:second message'])
```

      New in version 3.4.

   There are also other methods used to perform more specific checks, such as:

   +-----------------------------------------+----------------------------------+----------------+
   | Method                                  | Checks that                      | New in         |
   |=========================================|==================================|================|
   | |:unittest.TestCase.assertAlmostEqual:  | "round(a-b, 7) == 0"             |                |
   | assertAlmostEqual(a, b)|                |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertNotAlmostEqu  | "round(a-b, 7) != 0"             |                |
   | al:assertNotAlmostEqual(a, b)|          |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertGreater:asse  | "a > b"                          | 3.1            |
   | rtGreater(a, b)|                        |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertGreaterEqual  | "a >= b"                         | 3.1            |
   | :assertGreaterEqual(a, b)|              |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertLess:assertL  | "a < b"                          | 3.1            |
   | ess(a, b)|                              |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertLessEqual:as  | "a <= b"                         | 3.1            |
   | sertLessEqual(a, b)|                    |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertRegex:assert  | "r.search(s)"                    | 3.1            |
   | Regex(s, r)|                            |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertNotRegex:ass  | "not r.search(s)"                | 3.2            |
   | ertNotRegex(s, r)|                      |                                  |                |
   +-----------------------------------------+----------------------------------+----------------+
   | |:unittest.TestCase.assertCountEqual:a  | *a* and *b* have the same        | 3.2            |
   | ssertCountEqual(a, b)|                  | elements in the same number,     |                |
   |                                         | regardless of their order.       |                |
   +-----------------------------------------+----------------------------------+----------------+

   *unittest.TestCase.assertAlmostEqual:assertAlmostEqual(first, second, places=7, msg=None, delta=None)*
   *unittest.TestCase.assertNotAlmostEqual:assertNotAlmostEqual(first, second, places=7, msg=None, delta=None)*

      Test that *first* and *second* are approximately (or not approximately) equal by
      computing the difference, rounding to the given number of decimal *places*
      (default 7), and comparing to zero.  Note that these methods round the values to
      the given number of *decimal places* (i.e. like the |:library/functions.txt/round:round()| function) and not
      *significant digits*.

      If *delta* is supplied instead of *places* then the difference between *first*
      and *second* must be less or equal to (or greater than) *delta*.

      Supplying both *delta* and *places* raises a |:library/exceptions.txt/TypeError:TypeError|.

      Changed in version 3.2: |:unittest.TestCase.assertAlmostEqual:assertAlmostEqual()| automatically considers almost
      equal objects that compare equal.  |:unittest.TestCase.assertNotAlmostEqual:assertNotAlmostEqual()| automatically fails
      if the objects compare equal.  Added the *delta* keyword argument.

   *unittest.TestCase.assertGreater:assertGreater(first, second, msg=None)*
   *unittest.TestCase.assertGreaterEqual:assertGreaterEqual(first, second, msg=None)*
   *unittest.TestCase.assertLess:assertLess(first, second, msg=None)*
   *unittest.TestCase.assertLessEqual:assertLessEqual(first, second, msg=None)*

      Test that *first* is respectively >, >=, < or <= than *second* depending on the
      method name.  If not, the test will fail:

```rst
>>> self.assertGreaterEqual(3, 4)
AssertionError: "3" unexpectedly not greater than or equal to "4"
```

      New in version 3.1.

   *unittest.TestCase.assertRegex:assertRegex(text, regex, msg=None)*
   *unittest.TestCase.assertNotRegex:assertNotRegex(text, regex, msg=None)*

      Test that a *regex* search matches (or does not match) *text*.  In case of
      failure, the error message will include the pattern and the *text* (or the
      pattern and the part of *text* that unexpectedly matched).  *regex* may be a
      regular expression object or a string containing a regular expression suitable
      for use by |:library/re.txt/re.search:re.search()|.

      New in version 3.1: Added under the name "assertRegexpMatches".

      Changed in version 3.2: The method "assertRegexpMatches()" has been renamed to
      |:unittest.TestCase.assertRegex:assertRegex()|.

      New in version 3.2: |:unittest.TestCase.assertNotRegex:assertNotRegex()|.

      New in version 3.5: The name "assertNotRegexpMatches" is a deprecated alias for
      |:unittest.TestCase.assertNotRegex:assertNotRegex()|.

   *unittest.TestCase.assertCountEqual:assertCountEqual(first, second, msg=None)*

      Test that sequence *first* contains the same elements as *second*, regardless of
      their order. When they don’t, an error message listing the differences between
      the sequences will be generated.

      Duplicate elements are *not* ignored when comparing *first* and *second*. It
      verifies whether each element has the same count in both sequences. Equivalent
      to: "assertEqual(Counter(list(first)), Counter(list(second)))" but works with
      sequences of unhashable objects as well.

      New in version 3.2.

   *|type-specific-methods:⚓|*

   The |:unittest.TestCase.assertEqual:assertEqual()| method dispatches the equality check for objects of the same
   type to different type-specific methods.  These methods are already implemented
   for most of the built-in types, but it’s also possible to register new methods
   using |:unittest.TestCase.addTypeEqualityFunc:addTypeEqualityFunc()|:

   *unittest.TestCase.addTypeEqualityFunc:addTypeEqualityFunc(typeobj, function)*

      Registers a type-specific method called by |:unittest.TestCase.assertEqual:assertEqual()| to check if two
      objects of exactly the same *typeobj* (not subclasses) compare equal.  *function
      * must take two positional arguments and a third msg=None keyword argument just
      as |:unittest.TestCase.assertEqual:assertEqual()| does.  It must raise |:unittest.TestCase.failureException:self.failureException(msg)| when
      inequality between the first two parameters is detected – possibly providing
      useful information and explaining the inequalities in details in the error
      message.

      New in version 3.1.

   The list of type-specific methods automatically used by |:unittest.TestCase.assertEqual:assertEqual()| are
   summarized in the following table.  Note that it’s usually not necessary to
   invoke these methods directly.

   +-------------------------------------------+-------------------------------+----------------+
   | Method                                    | Used to compare               | New in         |
   |===========================================|===============================|================|
   | |:unittest.TestCase.assertMultiLineEqual  | strings                       | 3.1            |
   | :assertMultiLineEqual(a, b)|              |                               |                |
   +-------------------------------------------+-------------------------------+----------------+
   | |:unittest.TestCase.assertSequenceEqual:  | sequences                     | 3.1            |
   | assertSequenceEqual(a, b)|                |                               |                |
   +-------------------------------------------+-------------------------------+----------------+
   | |:unittest.TestCase.assertListEqual:asse  | lists                         | 3.1            |
   | rtListEqual(a, b)|                        |                               |                |
   +-------------------------------------------+-------------------------------+----------------+
   | |:unittest.TestCase.assertTupleEqual:ass  | tuples                        | 3.1            |
   | ertTupleEqual(a, b)|                      |                               |                |
   +-------------------------------------------+-------------------------------+----------------+
   | |:unittest.TestCase.assertSetEqual:asser  | sets or frozensets            | 3.1            |
   | tSetEqual(a, b)|                          |                               |                |
   +-------------------------------------------+-------------------------------+----------------+
   | |:unittest.TestCase.assertDictEqual:asse  | dicts                         | 3.1            |
   | rtDictEqual(a, b)|                        |                               |                |
   +-------------------------------------------+-------------------------------+----------------+

   *unittest.TestCase.assertMultiLineEqual:assertMultiLineEqual(first, second, msg=None)*

      Test that the multiline string *first* is equal to the string *second*. When not
      equal a diff of the two strings highlighting the differences will be included in
      the error message. This method is used by default when comparing strings with
      |:unittest.TestCase.assertEqual:assertEqual()|.

      New in version 3.1.

   *unittest.TestCase.assertSequenceEqual:assertSequenceEqual(first, second, msg=None, seq_type=None)*

      Tests that two sequences are equal.  If a *seq_type* is supplied, both *first*
      and *second* must be instances of *seq_type* or a failure will be raised.  If
      the sequences are different an error message is constructed that shows the
      difference between the two.

      This method is not called directly by |:unittest.TestCase.assertEqual:assertEqual()|, but it’s used to
      implement |:unittest.TestCase.assertListEqual:assertListEqual()| and |:unittest.TestCase.assertTupleEqual:assertTupleEqual()|.

      New in version 3.1.

   *unittest.TestCase.assertListEqual:assertListEqual(first, second, msg=None)*
   *unittest.TestCase.assertTupleEqual:assertTupleEqual(first, second, msg=None)*

      Tests that two lists or tuples are equal.  If not, an error message is
      constructed that shows only the differences between the two.  An error is also
      raised if either of the parameters are of the wrong type. These methods are used
      by default when comparing lists or tuples with |:unittest.TestCase.assertEqual:assertEqual()|.

      New in version 3.1.

   *unittest.TestCase.assertSetEqual:assertSetEqual(first, second, msg=None)*

      Tests that two sets are equal.  If not, an error message is constructed that
      lists the differences between the sets.  This method is used by default when
      comparing sets or frozensets with |:unittest.TestCase.assertEqual:assertEqual()|.

      Fails if either of *first* or *second* does not have a "set.difference()"
      method.

      New in version 3.1.

   *unittest.TestCase.assertDictEqual:assertDictEqual(first, second, msg=None)*

      Test that two dictionaries are equal.  If not, an error message is constructed
      that shows the differences in the dictionaries. This method will be used by
      default to compare dictionaries in calls to |:unittest.TestCase.assertEqual:assertEqual()|.

      New in version 3.1.

   *|other-methods-and-attrs:⚓|*

   Finally the |:unittest.TestCase:TestCase| provides the following methods and attributes:

   *unittest.TestCase.fail:fail(msg=None)*

      Signals a test failure unconditionally, with *msg* or "None" for the error
      message.

   *unittest.TestCase.failureException:failureException*

      This class attribute gives the exception raised by the test method.  If a test
      framework needs to use a specialized exception, possibly to carry additional
      information, it must subclass this exception in order to “play fair” with the
      framework.  The initial value of this attribute is |:library/exceptions.txt/AssertionError:AssertionError|.

   *unittest.TestCase.longMessage:longMessage*

      This class attribute determines what happens when a custom failure message is
      passed as the msg argument to an assertXYY call that fails. "True" is the
      default value. In this case, the custom message is appended to the end of the
      standard failure message. When set to "False", the custom message replaces the
      standard message.

      The class setting can be overridden in individual test methods by assigning an
      instance attribute, self.longMessage, to "True" or "False" before calling the
      assert methods.

      The class setting gets reset before each test call.

      New in version 3.1.

   *unittest.TestCase.maxDiff:maxDiff*

      This attribute controls the maximum length of diffs output by assert methods
      that report diffs on failure. It defaults to 80*8 characters. Assert methods
      affected by this attribute are |:unittest.TestCase.assertSequenceEqual:assertSequenceEqual()| (including all the
      sequence comparison methods that delegate to it), |:unittest.TestCase.assertDictEqual:assertDictEqual()| and
      |:unittest.TestCase.assertMultiLineEqual:assertMultiLineEqual()|.

      Setting "maxDiff" to "None" means that there is no maximum length of diffs.

      New in version 3.2.

   Testing frameworks can use the following methods to collect information on the
   test:

   *unittest.TestCase.countTestCases:countTestCases()*

      Return the number of tests represented by this test object.  For |:unittest.TestCase:TestCase|
      instances, this will always be "1".

   *unittest.TestCase.defaultTestResult:defaultTestResult()*

      Return an instance of the test result class that should be used for this test
      case class (if no other result instance is provided to the |:unittest.TestCase.run:run()| method).

      For |:unittest.TestCase:TestCase| instances, this will always be an instance of |:unittest.TestResult:TestResult|;
      subclasses of |:unittest.TestCase:TestCase| should override this as necessary.

   *unittest.TestCase.id:id()*

      Return a string identifying the specific test case.  This is usually the full
      name of the test method, including the module and class name.

   *unittest.TestCase.shortDescription:shortDescription()*

      Returns a description of the test, or "None" if no description has been
      provided.  The default implementation of this method returns the first line of
      the test method’s docstring, if available, or "None".

      Changed in version 3.1: In 3.1 this was changed to add the test name to the
      short description even in the presence of a docstring.  This caused
      compatibility issues with unittest extensions and adding the test name was moved
      to the |:unittest.TextTestResult:TextTestResult| in Python 3.2.

   *unittest.TestCase.addCleanup:addCleanup(function, *args, **kwargs)*

      Add a function to be called after |:unittest.TestCase.tearDown:tearDown()| to cleanup resources used during
      the test. Functions will be called in reverse order to the order they are added
      (LIFO (last-in, first-out)).  They are called with any arguments and keyword
      arguments passed into |:unittest.TestCase.addCleanup:addCleanup()| when they are added.

      If |:unittest.TestCase.setUp:setUp()| fails, meaning that |:unittest.TestCase.tearDown:tearDown()| is not called, then any cleanup
      functions added will still be called.

      New in version 3.1.

   *unittest.TestCase.doCleanups:doCleanups()*

      This method is called unconditionally after |:unittest.TestCase.tearDown:tearDown()|, or after |:unittest.TestCase.setUp:setUp()| if
      |:unittest.TestCase.setUp:setUp()| raises an exception.

      It is responsible for calling all the cleanup functions added by |:unittest.TestCase.addCleanup:addCleanup()|.
      If you need cleanup functions to be called *prior* to |:unittest.TestCase.tearDown:tearDown()| then you can
      call |:unittest.TestCase.doCleanups:doCleanups()| yourself.

      |:unittest.TestCase.doCleanups:doCleanups()| pops methods off the stack of cleanup functions one at a time, so
      it can be called at any time.

      New in version 3.1.

   *unittest.TestCase.addClassCleanup:classmethod addClassCleanup(function, /, *args, **kwargs)*

      Add a function to be called after |:unittest.TestCase.tearDownClass:tearDownClass()| to cleanup resources used
      during the test class. Functions will be called in reverse order to the order
      they are added (LIFO (last-in, first-out)). They are called with any arguments
      and keyword arguments passed into |:unittest.TestCase.addClassCleanup:addClassCleanup()| when they are added.

      If |:unittest.TestCase.setUpClass:setUpClass()| fails, meaning that |:unittest.TestCase.tearDownClass:tearDownClass()| is not called, then any
      cleanup functions added will still be called.

      New in version 3.8.

   *unittest.TestCase.doClassCleanups:classmethod doClassCleanups()*

      This method is called unconditionally after |:unittest.TestCase.tearDownClass:tearDownClass()|, or after
      |:unittest.TestCase.setUpClass:setUpClass()| if |:unittest.TestCase.setUpClass:setUpClass()| raises an exception.

      It is responsible for calling all the cleanup functions added by
      |:unittest.TestCase.addClassCleanup:addClassCleanup()|. If you need cleanup functions to be called *prior* to
      |:unittest.TestCase.tearDownClass:tearDownClass()| then you can call |:unittest.TestCase.doClassCleanups:doClassCleanups()| yourself.

      |:unittest.TestCase.doClassCleanups:doClassCleanups()| pops methods off the stack of cleanup functions one at a
      time, so it can be called at any time.

      New in version 3.8.

*unittest.IsolatedAsyncioTestCase:class unittest.IsolatedAsyncioTestCase(methodName='runTest')*

   This class provides an API similar to |:unittest.TestCase:TestCase| and also accepts coroutines as
   test functions.

   New in version 3.8.

   *unittest.IsolatedAsyncioTestCase.asyncSetUp:coroutine asyncSetUp()*

      Method called to prepare the test fixture. This is called after "setUp()". This
      is called immediately before calling the test method; other than
      |:library/exceptions.txt/AssertionError:AssertionError| or |:unittest.SkipTest:SkipTest|, any exception raised by this method will be
      considered an error rather than a test failure. The default implementation does
      nothing.

   *unittest.IsolatedAsyncioTestCase.asyncTearDown:coroutine asyncTearDown()*

      Method called immediately after the test method has been called and the result
      recorded.  This is called before "tearDown()". This is called even if the test
      method raised an exception, so the implementation in subclasses may need to be
      particularly careful about checking internal state.  Any exception, other than
      |:library/exceptions.txt/AssertionError:AssertionError| or |:unittest.SkipTest:SkipTest|, raised by this method will be considered an
      additional error rather than a test failure (thus increasing the total number of
      reported errors). This method will only be called if the |:unittest.IsolatedAsyncioTestCase.asyncSetUp:asyncSetUp()|
      succeeds, regardless of the outcome of the test method. The default
      implementation does nothing.

   *unittest.IsolatedAsyncioTestCase.addAsyncCleanup:addAsyncCleanup(function, /, *args, **kwargs)*

      This method accepts a coroutine that can be used as a cleanup function.

   *unittest.IsolatedAsyncioTestCase.run:run(result=None)*

      Sets up a new event loop to run the test, collecting the result into the
      |:unittest.TestResult:TestResult| object passed as *result*.  If *result* is omitted or "None", a
      temporary result object is created (by calling the "defaultTestResult()" method)
      and used. The result object is returned to |:unittest.IsolatedAsyncioTestCase.run:run()|’s caller. At the end of the
      test all the tasks in the event loop are cancelled.

   An example illustrating the order:

```rst
from unittest import IsolatedAsyncioTestCase

events = []


class Test(IsolatedAsyncioTestCase):


    def setUp(self):
        events.append("setUp")

    async def asyncSetUp(self):
        self._async_connection = await AsyncConnection()
        events.append("asyncSetUp")

    async def test_response(self):
        events.append("test_response")
        response = await self._async_connection.get("https://example.com")
        self.assertEqual(response.status_code, 200)
        self.addAsyncCleanup(self.on_cleanup)

    def tearDown(self):
        events.append("tearDown")

    async def asyncTearDown(self):
        await self._async_connection.close()
        events.append("asyncTearDown")

    async def on_cleanup(self):
        events.append("cleanup")

if __name__ == "__main__":
    unittest.main()
```

   After running the test, "events" would contain "["setUp", "asyncSetUp",
   "test_response", "asyncTearDown", "tearDown", "cleanup"]".

*unittest.FunctionTestCase:class unittest.FunctionTestCase(testFunc, setUp=None, tearDown=None, description=None)*

   This class implements the portion of the |:unittest.TestCase:TestCase| interface which allows the
   test runner to drive the test, but does not provide the methods which test code
   can use to check and report errors.  This is used to create test cases using
   legacy test code, allowing it to be integrated into a |:module-unittest:unittest|-based test
   framework.

### deprecated-aliases:Deprecated aliases

For historical reasons, some of the |:unittest.TestCase:TestCase| methods had one or more aliases
that are now deprecated.  The following table lists the correct names along with
their deprecated aliases:

   +--------------------------------+------------------------+-------------------------+
   | Method Name                    | Deprecated alias       | Deprecated alias        |
   |================================|========================|=========================|
   | |:unittest.TestCase.assertEqu  | failUnlessEqual        | assertEquals            |
   | al:assertEqual()|              |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertNot  | failIfEqual            | assertNotEquals         |
   | Equal:assertNotEqual()|        |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertTru  | failUnless             | assert_                 |
   | e:assertTrue()|                |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertFal  | failIf                 |                         |
   | se:assertFalse()|              |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertRai  | failUnlessRaises       |                         |
   | ses:assertRaises()|            |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertAlm  | failUnlessAlmostEqual  | assertAlmostEquals      |
   | ostEqual:assertAlmostEqual()|  |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertNot  | failIfAlmostEqual      | assertNotAlmostEquals   |
   | AlmostEqual:assertNotAlmostEq  |                        |                         |
   | ual()|                         |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertReg  |                        | assertRegexpMatches     |
   | ex:assertRegex()|              |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertNot  |                        | assertNotRegexpMatches  |
   | Regex:assertNotRegex()|        |                        |                         |
   +--------------------------------+------------------------+-------------------------+
   | |:unittest.TestCase.assertRai  |                        | assertRaisesRegexp      |
   | sesRegex:assertRaisesRegex()|  |                        |                         |
   +--------------------------------+------------------------+-------------------------+

   Deprecated since version 3.1: The fail* aliases listed in the second column have
   been deprecated.

   Deprecated since version 3.2: The assert* aliases listed in the third column
   have been deprecated.

   Deprecated since version 3.2: "assertRegexpMatches" and "assertRaisesRegexp"
   have been renamed to |:unittest.TestCase.assertRegex:assertRegex()| and |:unittest.TestCase.assertRaisesRegex:assertRaisesRegex()|.

   Deprecated since version 3.5: The "assertNotRegexpMatches" name is deprecated in
   favor of |:unittest.TestCase.assertNotRegex:assertNotRegex()|.

*|testsuite-objects:⚓|* ## grouping-tests:Grouping tests

*unittest.TestSuite:class unittest.TestSuite(tests=())*

   This class represents an aggregation of individual test cases and test suites.
   The class presents the interface needed by the test runner to allow it to be run
   as any other test case.  Running a |:unittest.TestSuite:TestSuite| instance is the same as iterating
   over the suite, running each test individually.

   If *tests* is given, it must be an iterable of individual test cases or other
   test suites that will be used to build the suite initially. Additional methods
   are provided to add test cases and suites to the collection later on.

   |:unittest.TestSuite:TestSuite| objects behave much like |:unittest.TestCase:TestCase| objects, except they do not
   actually implement a test.  Instead, they are used to aggregate tests into
   groups of tests that should be run together. Some additional methods are
   available to add tests to |:unittest.TestSuite:TestSuite| instances:

   *unittest.TestSuite.addTest:addTest(test)*

      Add a |:unittest.TestCase:TestCase| or |:unittest.TestSuite:TestSuite| to the suite.

   *unittest.TestSuite.addTests:addTests(tests)*

      Add all the tests from an iterable of |:unittest.TestCase:TestCase| and |:unittest.TestSuite:TestSuite| instances to
      this test suite.

      This is equivalent to iterating over *tests*, calling |:unittest.TestSuite.addTest:addTest()| for each
      element.

   |:unittest.TestSuite:TestSuite| shares the following methods with |:unittest.TestCase:TestCase|:

   *unittest.TestSuite.run:run(result)*

      Run the tests associated with this suite, collecting the result into the test
      result object passed as *result*.  Note that unlike |:unittest.TestCase.run:TestCase.run()|,
      |:unittest.TestSuite.run:TestSuite.run()| requires the result object to be passed in.

   *unittest.TestSuite.debug:debug()*

      Run the tests associated with this suite without collecting the result. This
      allows exceptions raised by the test to be propagated to the caller and can be
      used to support running tests under a debugger.

   *unittest.TestSuite.countTestCases:countTestCases()*

      Return the number of tests represented by this test object, including all
      individual tests and sub-suites.

   *unittest.TestSuite.__iter__:__iter__()*

      Tests grouped by a |:unittest.TestSuite:TestSuite| are always accessed by iteration. Subclasses can
      lazily provide tests by overriding |:unittest.TestSuite.__iter__:__iter__()|. Note that this method may be
      called several times on a single suite (for example when counting tests or
      comparing for equality) so the tests returned by repeated iterations before
      |:unittest.TestSuite.run:TestSuite.run()| must be the same for each call iteration. After
      |:unittest.TestSuite.run:TestSuite.run()|, callers should not rely on the tests returned by this method
      unless the caller uses a subclass that overrides "TestSuite._removeTestAtIndex()
      " to preserve test references.

      Changed in version 3.2: In earlier versions the |:unittest.TestSuite:TestSuite| accessed tests
      directly rather than through iteration, so overriding |:unittest.TestSuite.__iter__:__iter__()| wasn’t
      sufficient for providing tests.

      Changed in version 3.4: In earlier versions the |:unittest.TestSuite:TestSuite| held references to
      each |:unittest.TestCase:TestCase| after |:unittest.TestSuite.run:TestSuite.run()|. Subclasses can restore that behavior by
      overriding "TestSuite._removeTestAtIndex()".

   In the typical usage of a |:unittest.TestSuite:TestSuite| object, the |:unittest.TestSuite.run:run()| method is invoked by a
   "TestRunner" rather than by the end-user test harness.

## loading-and-running-tests:Loading and running tests

*unittest.TestLoader:class unittest.TestLoader*

   The |:unittest.TestLoader:TestLoader| class is used to create test suites from classes and modules.
   Normally, there is no need to create an instance of this class; the |:module-unittest:unittest|
   module provides an instance that can be shared as |:unittest.defaultTestLoader:unittest.defaultTestLoader|.
   Using a subclass or instance, however, allows customization of some configurable
   properties.

   |:unittest.TestLoader:TestLoader| objects have the following attributes:

   *unittest.TestLoader.errors:errors*

      A list of the non-fatal errors encountered while loading tests. Not reset by the
      loader at any point. Fatal errors are signalled by the relevant a method raising
      an exception to the caller. Non-fatal errors are also indicated by a synthetic
      test that will raise the original error when run.

      New in version 3.5.

   |:unittest.TestLoader:TestLoader| objects have the following methods:

   *unittest.TestLoader.loadTestsFromTestCase:loadTestsFromTestCase(testCaseClass)*

      Return a suite of all test cases contained in the |:unittest.TestCase:TestCase|-derived "
      testCaseClass".

      A test case instance is created for each method named by |:unittest.TestLoader.getTestCaseNames:getTestCaseNames()|.
      By default these are the method names beginning with "test". If
      |:unittest.TestLoader.getTestCaseNames:getTestCaseNames()| returns no methods, but the "runTest()" method is
      implemented, a single test case is created for that method instead.

   *unittest.TestLoader.loadTestsFromModule:loadTestsFromModule(module, pattern=None)*

      Return a suite of all test cases contained in the given module. This method
      searches *module* for classes derived from |:unittest.TestCase:TestCase| and creates an instance of
      the class for each test method defined for the class.

      Note:

        While using a hierarchy of |:unittest.TestCase:TestCase|-derived classes can be convenient in
        sharing fixtures and helper functions, defining test methods on base classes
        that are not intended to be instantiated directly does not play well with this
        method.  Doing so, however, can be useful when the fixtures are different and
        defined in subclasses.

      If a module provides a "load_tests" function it will be called to load the
      tests. This allows modules to customize test loading. This is the
      |:load-tests-protocol:load_tests protocol|.  The *pattern* argument is passed as the third argument
      to "load_tests".

      Changed in version 3.2: Support for "load_tests" added.

      Changed in version 3.5: The undocumented and unofficial *use_load_tests* default
      argument is deprecated and ignored, although it is still accepted for backward
      compatibility.  The method also now accepts a keyword-only argument *pattern*
      which is passed to "load_tests" as the third argument.

   *unittest.TestLoader.loadTestsFromName:loadTestsFromName(name, module=None)*

      Return a suite of all test cases given a string specifier.

      The specifier *name* is a “dotted name” that may resolve either to a module, a
      test case class, a test method within a test case class, a |:unittest.TestSuite:TestSuite| instance,
      or a callable object which returns a |:unittest.TestCase:TestCase| or |:unittest.TestSuite:TestSuite| instance.  These
      checks are applied in the order listed here; that is, a method on a possible
      test case class will be picked up as “a test method within a test case class”,
      rather than “a callable object”.

      For example, if you have a module "SampleTests" containing a |:unittest.TestCase:TestCase|-derived
      class "SampleTestCase" with three test methods ("test_one()", "test_two()", and
      "test_three()"), the specifier "'SampleTests.SampleTestCase'" would cause this
      method to return a suite which will run all three test methods. Using the
      specifier "'SampleTests.SampleTestCase.test_two'" would cause it to return a
      test suite which will run only the "test_two()" test method. The specifier can
      refer to modules and packages which have not been imported; they will be
      imported as a side-effect.

      The method optionally resolves *name* relative to the given *module*.

      Changed in version 3.5: If an |:library/exceptions.txt/ImportError:ImportError| or |:library/exceptions.txt/AttributeError:AttributeError| occurs while
      traversing *name* then a synthetic test that raises that error when run will be
      returned. These errors are included in the errors accumulated by self.errors.

   *unittest.TestLoader.loadTestsFromNames:loadTestsFromNames(names, module=None)*

      Similar to |:unittest.TestLoader.loadTestsFromName:loadTestsFromName()|, but takes a sequence of names rather than a
      single name.  The return value is a test suite which supports all the tests
      defined for each name.

   *unittest.TestLoader.getTestCaseNames:getTestCaseNames(testCaseClass)*

      Return a sorted sequence of method names found within *testCaseClass*; this
      should be a subclass of |:unittest.TestCase:TestCase|.

   *unittest.TestLoader.discover:discover(start_dir, pattern='test*.py', top_level_dir=None)*

      Find all the test modules by recursing into subdirectories from the specified
      start directory, and return a TestSuite object containing them. Only test files
      that match *pattern* will be loaded. (Using shell style pattern matching.) Only
      module names that are importable (i.e. are valid Python identifiers) will be
      loaded.

      All test modules must be importable from the top level of the project. If the
      start directory is not the top level directory then the top level directory must
      be specified separately.

      If importing a module fails, for example due to a syntax error, then this will
      be recorded as a single error and discovery will continue.  If the import
      failure is due to |:unittest.SkipTest:SkipTest| being raised, it will be recorded as a skip instead
      of an error.

      If a package (a directory containing a file named "__init__.py") is found, the
      package will be checked for a "load_tests" function. If this exists then it will
      be called "package.load_tests(loader, tests, pattern)". Test discovery takes
      care to ensure that a package is only checked for tests once during an
      invocation, even if the load_tests function itself calls "loader.discover".

      If "load_tests" exists then discovery does *not* recurse into the package, "
      load_tests" is responsible for loading all tests in the package.

      The pattern is deliberately not stored as a loader attribute so that packages
      can continue discovery themselves. *top_level_dir* is stored so "load_tests"
      does not need to pass this argument in to "loader.discover()".

      *start_dir* can be a dotted module name as well as a directory.

      New in version 3.2.

      Changed in version 3.4: Modules that raise |:unittest.SkipTest:SkipTest| on import are recorded as
      skips,   not errors. Discovery works for |:glossary.txt/term-namespace-package:namespace packages|. Paths are sorted
      before being imported so that execution order is   the same even if the
      underlying file system’s ordering is not   dependent on file name.

      Changed in version 3.5: Found packages are now checked for "load_tests"
      regardless of whether their path matches *pattern*, because it is impossible for
      a package name to match the default pattern.

   The following attributes of a |:unittest.TestLoader:TestLoader| can be configured either by
   subclassing or assignment on an instance:

   *unittest.TestLoader.testMethodPrefix:testMethodPrefix*

      String giving the prefix of method names which will be interpreted as test
      methods.  The default value is "'test'".

      This affects |:unittest.TestLoader.getTestCaseNames:getTestCaseNames()| and all the "loadTestsFrom*()" methods.

   *unittest.TestLoader.sortTestMethodsUsing:sortTestMethodsUsing*

      Function to be used to compare method names when sorting them in
      |:unittest.TestLoader.getTestCaseNames:getTestCaseNames()| and all the "loadTestsFrom*()" methods.

   *unittest.TestLoader.suiteClass:suiteClass*

      Callable object that constructs a test suite from a list of tests. No methods on
      the resulting object are needed.  The default value is the |:unittest.TestSuite:TestSuite| class.

      This affects all the "loadTestsFrom*()" methods.

   *unittest.TestLoader.testNamePatterns:testNamePatterns*

      List of Unix shell-style wildcard test name patterns that test methods have to
      match to be included in test suites (see "-v" option).

      If this attribute is not "None" (the default), all test methods to be included
      in test suites must match one of the patterns in this list. Note that matches
      are always performed using |:library/fnmatch.txt/fnmatch.fnmatchcase:fnmatch.fnmatchcase()|, so unlike patterns passed to
      the "-v" option, simple substring patterns will have to be converted using "*"
      wildcards.

      This affects all the "loadTestsFrom*()" methods.

      New in version 3.7.

*unittest.TestResult:class unittest.TestResult*

   This class is used to compile information about which tests have succeeded and
   which have failed.

   A |:unittest.TestResult:TestResult| object stores the results of a set of tests.  The |:unittest.TestCase:TestCase| and
   |:unittest.TestSuite:TestSuite| classes ensure that results are properly recorded; test authors do
   not need to worry about recording the outcome of tests.

   Testing frameworks built on top of |:module-unittest:unittest| may want access to the
   |:unittest.TestResult:TestResult| object generated by running a set of tests for reporting purposes;
   a |:unittest.TestResult:TestResult| instance is returned by the "TestRunner.run()" method for this
   purpose.

   |:unittest.TestResult:TestResult| instances have the following attributes that will be of interest
   when inspecting the results of running a set of tests:

   *unittest.TestResult.errors:errors*

      A list containing 2-tuples of |:unittest.TestCase:TestCase| instances and strings holding formatted
      tracebacks. Each tuple represents a test which raised an unexpected exception.

   *unittest.TestResult.failures:failures*

      A list containing 2-tuples of |:unittest.TestCase:TestCase| instances and strings holding formatted
      tracebacks. Each tuple represents a test where a failure was explicitly
      signalled using the "TestCase.assert*()" methods.

   *unittest.TestResult.skipped:skipped*

      A list containing 2-tuples of |:unittest.TestCase:TestCase| instances and strings holding the
      reason for skipping the test.

      New in version 3.1.

   *unittest.TestResult.expectedFailures:expectedFailures*

      A list containing 2-tuples of |:unittest.TestCase:TestCase| instances and strings holding formatted
      tracebacks.  Each tuple represents an expected failure or error of the test
      case.

   *unittest.TestResult.unexpectedSuccesses:unexpectedSuccesses*

      A list containing |:unittest.TestCase:TestCase| instances that were marked as expected failures,
      but succeeded.

   *unittest.TestResult.shouldStop:shouldStop*

      Set to "True" when the execution of tests should stop by |:unittest.TestResult.stop:stop()|.

   *unittest.TestResult.testsRun:testsRun*

      The total number of tests run so far.

   *unittest.TestResult.buffer:buffer*

      If set to true, "sys.stdout" and "sys.stderr" will be buffered in between
      |:unittest.TestResult.startTest:startTest()| and |:unittest.TestResult.stopTest:stopTest()| being called. Collected output will only be
      echoed onto the real "sys.stdout" and "sys.stderr" if the test fails or errors.
      Any output is also attached to the failure / error message.

      New in version 3.2.

   *unittest.TestResult.failfast:failfast*

      If set to true |:unittest.TestResult.stop:stop()| will be called on the first failure or error, halting
      the test run.

      New in version 3.2.

   *unittest.TestResult.tb_locals:tb_locals*

      If set to true then local variables will be shown in tracebacks.

      New in version 3.5.

   *unittest.TestResult.wasSuccessful:wasSuccessful()*

      Return "True" if all tests run so far have passed, otherwise returns "False".

      Changed in version 3.4: Returns "False" if there were any |:unittest.TestResult.unexpectedSuccesses:unexpectedSuccesses|
      from tests marked with the |:unittest.expectedFailure:expectedFailure()| decorator.

   *unittest.TestResult.stop:stop()*

      This method can be called to signal that the set of tests being run should be
      aborted by setting the |:unittest.TestResult.shouldStop:shouldStop| attribute to "True". "TestRunner" objects
      should respect this flag and return without running any additional tests.

      For example, this feature is used by the |:unittest.TextTestRunner:TextTestRunner| class to stop the test
      framework when the user signals an interrupt from the keyboard.  Interactive
      tools which provide "TestRunner" implementations can use this in a similar
      manner.

   The following methods of the |:unittest.TestResult:TestResult| class are used to maintain the
   internal data structures, and may be extended in subclasses to support
   additional reporting requirements.  This is particularly useful in building
   tools which support interactive reporting while tests are being run.

   *unittest.TestResult.startTest:startTest(test)*

      Called when the test case *test* is about to be run.

   *unittest.TestResult.stopTest:stopTest(test)*

      Called after the test case *test* has been executed, regardless of the outcome.

   *unittest.TestResult.startTestRun:startTestRun()*

      Called once before any tests are executed.

      New in version 3.1.

   *unittest.TestResult.stopTestRun:stopTestRun()*

      Called once after all tests are executed.

      New in version 3.1.

   *unittest.TestResult.addError:addError(test, err)*

      Called when the test case *test* raises an unexpected exception. *err* is a
      tuple of the form returned by |:library/sys.txt/sys.exc_info:sys.exc_info()|: "(type, value, traceback)".

      The default implementation appends a tuple "(test, formatted_err)" to the
      instance’s |:unittest.TestResult.errors:errors| attribute, where *formatted_err* is a formatted traceback
      derived from *err*.

   *unittest.TestResult.addFailure:addFailure(test, err)*

      Called when the test case *test* signals a failure. *err* is a tuple of the form
      returned by |:library/sys.txt/sys.exc_info:sys.exc_info()|: "(type, value, traceback)".

      The default implementation appends a tuple "(test, formatted_err)" to the
      instance’s |:unittest.TestResult.failures:failures| attribute, where *formatted_err* is a formatted traceback
      derived from *err*.

   *unittest.TestResult.addSuccess:addSuccess(test)*

      Called when the test case *test* succeeds.

      The default implementation does nothing.

   *unittest.TestResult.addSkip:addSkip(test, reason)*

      Called when the test case *test* is skipped.  *reason* is the reason the test
      gave for skipping.

      The default implementation appends a tuple "(test, reason)" to the instance’s
      |:unittest.TestResult.skipped:skipped| attribute.

   *unittest.TestResult.addExpectedFailure:addExpectedFailure(test, err)*

      Called when the test case *test* fails or errors, but was marked with the
      |:unittest.expectedFailure:expectedFailure()| decorator.

      The default implementation appends a tuple "(test, formatted_err)" to the
      instance’s |:unittest.TestResult.expectedFailures:expectedFailures| attribute, where *formatted_err* is a formatted
      traceback derived from *err*.

   *unittest.TestResult.addUnexpectedSuccess:addUnexpectedSuccess(test)*

      Called when the test case *test* was marked with the |:unittest.expectedFailure:expectedFailure()|
      decorator, but succeeded.

      The default implementation appends the test to the instance’s
      |:unittest.TestResult.unexpectedSuccesses:unexpectedSuccesses| attribute.

   *unittest.TestResult.addSubTest:addSubTest(test, subtest, outcome)*

      Called when a subtest finishes.  *test* is the test case corresponding to the
      test method.  *subtest* is a custom |:unittest.TestCase:TestCase| instance describing the subtest.

      If *outcome* is |:library/constants.txt/None:None|, the subtest succeeded.  Otherwise, it failed with an
      exception where *outcome* is a tuple of the form returned by |:library/sys.txt/sys.exc_info:sys.exc_info()|: "
      (type, value, traceback)".

      The default implementation does nothing when the outcome is a success, and
      records subtest failures as normal failures.

      New in version 3.4.

*unittest.TextTestResult:class unittest.TextTestResult(stream, descriptions, verbosity)*

   A concrete implementation of |:unittest.TestResult:TestResult| used by the |:unittest.TextTestRunner:TextTestRunner|.

   New in version 3.2: This class was previously named "_TextTestResult". The old
   name still exists as an alias but is deprecated.

*unittest.defaultTestLoader:unittest.defaultTestLoader*

   Instance of the |:unittest.TestLoader:TestLoader| class intended to be shared.  If no customization
   of the |:unittest.TestLoader:TestLoader| is needed, this instance can be used instead of repeatedly
   creating new instances.

*unittest.TextTestRunner:class unittest.TextTestRunner(stream=None, descriptions=True, verbosity=1, failfast=False, buffer=False, resultclass=None, warnings=None, *, tb_locals=False)*

   A basic test runner implementation that outputs results to a stream. If *stream*
   is "None", the default, |:library/sys.txt/sys.stderr:sys.stderr| is used as the output stream. This class
   has a few configurable parameters, but is essentially very simple.  Graphical
   applications which run test suites should provide alternate implementations.
   Such implementations should accept "**kwargs" as the interface to construct
   runners changes when features are added to unittest.

   By default this runner shows |:library/exceptions.txt/DeprecationWarning:DeprecationWarning|, |:library/exceptions.txt/PendingDeprecationWarning:PendingDeprecationWarning|,
   |:library/exceptions.txt/ResourceWarning:ResourceWarning| and |:library/exceptions.txt/ImportWarning:ImportWarning| even if they are |:library/warnings.txt/warning-ignored:ignored by default|.
   Deprecation warnings caused by |:deprecated-aliases:deprecated unittest
methods| are also special-
   cased and, when the warning filters are "'default'" or "'always'", they will
   appear only once per-module, in order to avoid too many warning messages.  This
   behavior can be overridden using Python’s "-Wd" or "-Wa" options (see
   |:using/cmdline.txt/using-on-warnings:Warning control|) and leaving *warnings* to "None".

   Changed in version 3.2: Added the "warnings" argument.

   Changed in version 3.2: The default stream is set to |:library/sys.txt/sys.stderr:sys.stderr| at
   instantiation time rather than import time.

   Changed in version 3.5: Added the tb_locals parameter.

   *unittest.TextTestRunner._makeResult:_makeResult()*

      This method returns the instance of "TestResult" used by |:unittest.TextTestRunner.run:run()|. It is not
      intended to be called directly, but can be overridden in subclasses to provide a
      custom "TestResult".

      "_makeResult()" instantiates the class or callable passed in the "TextTestRunner
      " constructor as the "resultclass" argument. It defaults to |:unittest.TextTestResult:TextTestResult| if
      no "resultclass" is provided. The result class is instantiated with the
      following arguments:

```rst
stream, descriptions, verbosity
```

   *unittest.TextTestRunner.run:run(test)*

      This method is the main public interface to the "TextTestRunner". This method
      takes a |:unittest.TestSuite:TestSuite| or |:unittest.TestCase:TestCase| instance. A |:unittest.TestResult:TestResult| is created by calling
      |:unittest.TextTestRunner._makeResult:_makeResult()| and the test(s) are run and the results printed to stdout.

*unittest.main:unittest.main(module='__main__', defaultTest=None, argv=None, testRunner=None, testLoader=unittest.defaultTestLoader, exit=True, verbosity=1, failfast=None, catchbreak=None, buffer=None, warnings=None)*

   A command-line program that loads a set of tests from *module* and runs them;
   this is primarily for making test modules conveniently executable. The simplest
   use for this function is to include the following line at the end of a test
   script:

```rst
if __name__ == '__main__':
    unittest.main()
```

   You can run tests with more detailed information by passing in the verbosity
   argument:

```rst
if __name__ == '__main__':
    unittest.main(verbosity=2)
```

   The *defaultTest* argument is either the name of a single test or an iterable of
   test names to run if no test names are specified via *argv*.  If not specified
   or "None" and no test names are provided via *argv*, all tests found in *module*
   are run.

   The *argv* argument can be a list of options passed to the program, with the
   first element being the program name.  If not specified or "None", the values of
   |:library/sys.txt/sys.argv:sys.argv| are used.

   The *testRunner* argument can either be a test runner class or an already
   created instance of it. By default "main" calls |:library/sys.txt/sys.exit:sys.exit()| with an exit code
   indicating success or failure of the tests run.

   The *testLoader* argument has to be a |:unittest.TestLoader:TestLoader| instance, and defaults to
   |:unittest.defaultTestLoader:defaultTestLoader|.

   "main" supports being used from the interactive interpreter by passing in the
   argument "exit=False". This displays the result on standard output without
   calling |:library/sys.txt/sys.exit:sys.exit()|:

```rst
>>> from unittest import main
>>> main(module='test_module', exit=False)
```

   The *failfast*, *catchbreak* and *buffer* parameters have the same effect as the
   same-name |:command-line-options:command-line options|.

   The *warnings* argument specifies the |:library/warnings.txt/warning-filter:warning filter| that should be used while
   running the tests.  If it’s not specified, it will remain "None" if a "-W"
   option is passed to *python* (see |:using/cmdline.txt/using-on-warnings:Warning control|), otherwise it will be set
   to "'default'".

   Calling "main" actually returns an instance of the "TestProgram" class. This
   stores the result of the tests run as the "result" attribute.

   Changed in version 3.1: The *exit* parameter was added.

   Changed in version 3.2: The *verbosity*, *failfast*, *catchbreak*, *buffer* and
   *warnings* parameters were added.

   Changed in version 3.4: The *defaultTest* parameter was changed to also accept
   an iterable of test names.

### load-tests-protocol:load_tests Protocol

New in version 3.2.

Modules or packages can customize how tests are loaded from them during normal
test runs or test discovery by implementing a function called "load_tests".

If a test module defines "load_tests" it will be called by
|:unittest.TestLoader.loadTestsFromModule:TestLoader.loadTestsFromModule()| with the following arguments:

```rst
load_tests(loader, standard_tests, pattern)
```

where *pattern* is passed straight through from "loadTestsFromModule".  It
defaults to "None".

It should return a |:unittest.TestSuite:TestSuite|.

*loader* is the instance of |:unittest.TestLoader:TestLoader| doing the loading. *standard_tests* are
the tests that would be loaded by default from the module. It is common for test
modules to only want to add or remove tests from the standard set of tests. The
third argument is used when loading packages as part of test discovery.

A typical "load_tests" function that loads tests from a specific set of
|:unittest.TestCase:TestCase| classes may look like:

```rst
test_cases = (TestCase1, TestCase2, TestCase3)

def load_tests(loader, tests, pattern):
    suite = TestSuite()
    for test_class in test_cases:
        tests = loader.loadTestsFromTestCase(test_class)
        suite.addTests(tests)
    return suite
```

If discovery is started in a directory containing a package, either from the
command line or by calling |:unittest.TestLoader.discover:TestLoader.discover()|, then the package "
__init__.py" will be checked for "load_tests".  If that function does not exist,
discovery will recurse into the package as though it were just another
directory.  Otherwise, discovery of the package’s tests will be left up to "
load_tests" which is called with the following arguments:

```rst
load_tests(loader, standard_tests, pattern)
```

This should return a |:unittest.TestSuite:TestSuite| representing all the tests from the package. ("
standard_tests" will only contain tests collected from "__init__.py".)

Because the pattern is passed into "load_tests" the package is free to continue
(and potentially modify) test discovery. A ‘do nothing’ "load_tests" function
for a test package would look like:

```rst
def load_tests(loader, standard_tests, pattern):
    # top level directory cached on loader instance
    this_dir = os.path.dirname(__file__)
    package_tests = loader.discover(start_dir=this_dir, pattern=pattern)
    standard_tests.addTests(package_tests)
    return standard_tests
```

Changed in version 3.5: Discovery no longer checks package names for matching *
pattern* due to the impossibility of package names matching the default pattern.

# class-and-module-fixtures:Class and Module Fixtures

Class and module level fixtures are implemented in |:unittest.TestSuite:TestSuite|. When the test
suite encounters a test from a new class then "tearDownClass()" from the
previous class (if there is one) is called, followed by "setUpClass()" from the
new class.

Similarly if a test is from a different module from the previous test then "
tearDownModule" from the previous module is run, followed by "setUpModule" from
the new module.

After all the tests have run the final "tearDownClass" and "tearDownModule" are
run.

Note that shared fixtures do not play well with [potential] features like test
parallelization and they break test isolation. They should be used with care.

The default ordering of tests created by the unittest test loaders is to group
all tests from the same modules and classes together. This will lead to "
setUpClass" / "setUpModule" (etc) being called exactly once per class and
module. If you randomize the order, so that tests from different modules and
classes are adjacent to each other, then these shared fixture functions may be
called multiple times in a single test run.

Shared fixtures are not intended to work with suites with non-standard ordering.
A "BaseTestSuite" still exists for frameworks that don’t want to support shared
fixtures.

If there are any exceptions raised during one of the shared fixture functions
the test is reported as an error. Because there is no corresponding test
instance an "_ErrorHolder" object (that has the same interface as a |:unittest.TestCase:TestCase|)
is created to represent the error. If you are just using the standard unittest
test runner then this detail doesn’t matter, but if you are a framework author
it may be relevant.

## setupclass-and-teardownclass:setUpClass and tearDownClass

These must be implemented as class methods:

```rst
import unittest

class Test(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls._connection = createExpensiveConnectionObject()

    @classmethod
    def tearDownClass(cls):
        cls._connection.destroy()
```

If you want the "setUpClass" and "tearDownClass" on base classes called then you
must call up to them yourself. The implementations in |:unittest.TestCase:TestCase| are empty.

If an exception is raised during a "setUpClass" then the tests in the class are
not run and the "tearDownClass" is not run. Skipped classes will not have "
setUpClass" or "tearDownClass" run. If the exception is a |:unittest.SkipTest:SkipTest| exception
then the class will be reported as having been skipped instead of as an error.

## setupmodule-and-teardownmodule:setUpModule and tearDownModule

These should be implemented as functions:

```rst
def setUpModule():
    createConnection()

def tearDownModule():
    closeConnection()
```

If an exception is raised in a "setUpModule" then none of the tests in the
module will be run and the "tearDownModule" will not be run. If the exception is
a |:unittest.SkipTest:SkipTest| exception then the module will be reported as having been skipped
instead of as an error.

To add cleanup code that must be run even in the case of an exception, use "
addModuleCleanup":

*unittest.addModuleCleanup:unittest.addModuleCleanup(function, /, *args, **kwargs)*

   Add a function to be called after "tearDownModule()" to cleanup resources used
   during the test class. Functions will be called in reverse order to the order
   they are added (LIFO (last-in, first-out)). They are called with any arguments
   and keyword arguments passed into |:unittest.addModuleCleanup:addModuleCleanup()| when they are added.

   If "setUpModule()" fails, meaning that "tearDownModule()" is not called, then
   any cleanup functions added will still be called.

   New in version 3.8.

*unittest.doModuleCleanups:unittest.doModuleCleanups()*

   This function is called unconditionally after "tearDownModule()", or after "
   setUpModule()" if "setUpModule()" raises an exception.

   It is responsible for calling all the cleanup functions added by "
   addCleanupModule()". If you need cleanup functions to be called *prior* to "
   tearDownModule()" then you can call |:unittest.doModuleCleanups:doModuleCleanups()| yourself.

   |:unittest.doModuleCleanups:doModuleCleanups()| pops methods off the stack of cleanup functions one at a
   time, so it can be called at any time.

   New in version 3.8.

# signal-handling:Signal Handling

New in version 3.2.

The |:cmdoption-unittest-c:-c/--catch| command-line option to unittest, along with the "catchbreak"
parameter to |:unittest.main:unittest.main()|, provide more friendly handling of control-C
during a test run. With catch break behavior enabled control-C will allow the
currently running test to complete, and the test run will then end and report
all the results so far. A second control-c will raise a |:library/exceptions.txt/KeyboardInterrupt:KeyboardInterrupt| in
the usual way.

The control-c handling signal handler attempts to remain compatible with code or
tests that install their own |:library/signal.txt/signal.SIGINT:signal.SIGINT| handler. If the "unittest" handler
is called but *isn’t* the installed |:library/signal.txt/signal.SIGINT:signal.SIGINT| handler, i.e. it has been
replaced by the system under test and delegated to, then it calls the default
handler. This will normally be the expected behavior by code that replaces an
installed handler and delegates to it. For individual tests that need "unittest"
control-c handling disabled the |:unittest.removeHandler:removeHandler()| decorator can be used.

There are a few utility functions for framework authors to enable control-c
handling functionality within test frameworks.

*unittest.installHandler:unittest.installHandler()*

   Install the control-c handler. When a |:library/signal.txt/signal.SIGINT:signal.SIGINT| is received (usually in
   response to the user pressing control-c) all registered results have |:unittest.TestResult.stop:stop()|
   called.

*unittest.registerResult:unittest.registerResult(result)*

   Register a |:unittest.TestResult:TestResult| object for control-c handling. Registering a result
   stores a weak reference to it, so it doesn’t prevent the result from being
   garbage collected.

   Registering a |:unittest.TestResult:TestResult| object has no side-effects if control-c handling is
   not enabled, so test frameworks can unconditionally register all results they
   create independently of whether or not handling is enabled.

*unittest.removeResult:unittest.removeResult(result)*

   Remove a registered result. Once a result has been removed then |:unittest.TestResult.stop:stop()| will no
   longer be called on that result object in response to a control-c.

*unittest.removeHandler:unittest.removeHandler(function=None)*

   When called without arguments this function removes the control-c handler if it
   has been installed. This function can also be used as a test decorator to
   temporarily remove the handler while the test is being executed:

```rst
@unittest.removeHandler
def test_signal_handling(self):
    ...
```



